{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is running on Geforce GTX 1080Ti 12GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Basic Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # use first GPU\n",
    "\n",
    "seed = 2 # random seed\n",
    "model_dir = \"model_transfer/\" # folder for saving model and log\n",
    "resize_shape = (128, 128) # resized image size\n",
    "NUM_LABELS = 12 # number of labels\n",
    "BATCH_SIZE = 256 # number of images in one batch\n",
    "EPOCHS = 10\n",
    "SAVE_SUMMARY_STEPS = 100 # save summary to tensorboard - one step means one batch\n",
    "NUM_GPUS = 1 # number of GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "See Analyze.ipynb  \n",
    "dataset from: https://www.kaggle.com/c/plant-seedlings-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (4630, 128, 128, 3) (4630,)\n",
      "Validation set (120, 128, 128, 3) (120,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file = 'train_val10_{1}*{1}_seed{0}.pickle'.format(seed, resize_shape[0])\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    train_dataset = train_dataset.astype(\"float32\")\n",
    "    valid_dataset = valid_dataset.astype(\"float32\")\n",
    "    train_dataset /= 255\n",
    "    valid_dataset /= 255\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed',\n",
       "       'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize',\n",
       "       'Scentless Mayweed', 'Shepherds Purse',\n",
       "       'Small-flowered Cranesbill', 'Sugar beet'], dtype='<U25')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_labels = encoder.fit_transform(train_labels)\n",
    "valid_labels = encoder.transform(valid_labels)\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(train_dataset)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "generator = datagen.flow(train_dataset, train_labels,\n",
    "                         batch_size=1, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Training Data to Model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn_keras(generator):\n",
    "    gen_fn = lambda: generator\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(gen_fn, (tf.float32, tf.int64))\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.repeat(EPOCHS)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(None)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, labels = iterator.get_next()\n",
    "    features = tf.reshape(features, [-1, 128, 128, 3])\n",
    "    print(\"output feature:\", features.shape)\n",
    "    \n",
    "    tf.summary.image(\"images\", features)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Evaluation Data to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels):\n",
    "    \n",
    "    def make_generator(images, labels):\n",
    "\n",
    "        def _generator():\n",
    "            for image, label in zip(images, labels):\n",
    "                yield image, label\n",
    "\n",
    "        return _generator\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(make_generator(features, labels), (tf.float32, tf.int64))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, labels = iterator.get_next()\n",
    "    features = tf.reshape(features, [-1, 128, 128, 3])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "1. Tensorflow initail weight will use ```tf.global_variables_initializer()```, but it will destroy the pretrained weights. So save the pretrained weights\n",
    "2. Do not use keras way to get features on Keras pretrained model, i.e. ```features = model(feature)```. It will get duplicate graph  \n",
    "   Use the follwing code instead  \n",
    "   ```python\n",
    "   features = tf.identity(model.layers[-1].output, name='vgg16_output')\n",
    "   ```\n",
    "Reference: http://zachmoshe.com/2017/11/11/use-keras-models-with-tf.html \n",
    "2. Because we use Keras model, so running in Tensorflow original session will not get pretrained weights. Use ```tf.keras.backend.get_session()``` instead  \n",
    "Reference: https://github.com/keras-team/keras/issues/8438 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \"imagenet\"\n",
    "def vgg16(features, training):\n",
    "    global weights\n",
    "    \n",
    "    with tf.variable_scope(\"vgg16\"):\n",
    "        model = tf.keras.applications.VGG16(include_top=False, weights=weights, input_tensor=features, input_shape=(128, 128, 3))\n",
    "    # Only load imagenet weight on the first run\n",
    "    if weights==\"imagenet\":\n",
    "        sess = tf.keras.backend.get_session()\n",
    "        vgg_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='vgg16')\n",
    "        tf.train.Saver(vgg_weights).save(sess, \"pretrained_model/vgg16.ckpt\")\n",
    "        weights = None\n",
    "    features = tf.identity(model.layers[-1].output, name='vgg16_output')\n",
    "    features = tf.keras.layers.Flatten(name=\"flatten\")(features)\n",
    "    with tf.variable_scope(\"fine_tune\"):\n",
    "        features = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.relu, name=\"dense1\")(features)\n",
    "        features = tf.keras.layers.Dropout(rate=0.5, name=\"dropout1\")(features)\n",
    "        features = tf.keras.layers.Dense(units=128, activation=tf.keras.activations.relu, name=\"dense2\")(features)\n",
    "        features = tf.keras.layers.Dropout(rate=0.5, name=\"dropout2\")(features)\n",
    "        logits = tf.keras.layers.Dense(units=NUM_LABELS, name=\"output\")(features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function for tf.Estimator\n",
    "**Remember: set ```tf.keras.backend.set_learning_phase```. It will allow Keras model to update weights**  \n",
    "1. To save model for tensorflow serving, set **`export_outputs`** parameter in prediction mode\n",
    "2. We want to freeze the imagenet weights, update the fully connected layer  \n",
    "   Use follwing code instead  \n",
    "   ```python\n",
    "   train_op = optimizer.minimize(loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='fine_tune'), global_step=tf.train.get_global_step())\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # to save model for tensorflow serving\n",
    "    if isinstance(features, dict):\n",
    "        features = features['image']\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        training = True\n",
    "        tf.keras.backend.set_learning_phase(True)\n",
    "    else:\n",
    "        training = False\n",
    "        tf.keras.backend.set_learning_phase(False)\n",
    "    \n",
    "    logits = vgg16(features, training)\n",
    "    \n",
    "    predicted_class = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    # Prediction mode for tensorflow serving\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            \"class_ids\": predicted_class[:, tf.newaxis],\n",
    "            \"probability\": tf.nn.softmax(logits),\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs={ \n",
    "            'classify': tf.estimator.export.PredictOutput(predictions)})\n",
    "    \n",
    "    # calculate cross entropy loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    def _f1_score(labels, predictions, class_id):\n",
    "        \"\"\"\n",
    "        Reference: https://stackoverflow.com/questions/45603956/class-wise-precision-and-recall-for-multi-class-classification-in-tensorflow\n",
    "        \"\"\"\n",
    "        precision = tf.metrics.precision_at_k(labels, predictions, 1, class_id)\n",
    "        recall = tf.metrics.recall_at_k(labels, predictions, 1, class_id)\n",
    "        f1_score = 2 * (precision[0] * recall[0]) / (precision[0] + recall[0])\n",
    "        f1_score_op = 2 * (precision[1] * recall[1]) / (precision[1] + recall[1])\n",
    "        return (f1_score, f1_score_op)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_class, name=\"accuracy\")\n",
    "    f1_score_BlackGrass = _f1_score(labels=labels, predictions=logits, class_id=0)\n",
    "    metrics = {\"accuracy\" : accuracy, \"f1_score_BlackGrass\" : f1_score_BlackGrass}\n",
    "    tf.summary.scalar(\"accuracy\", accuracy[1])\n",
    "    tf.summary.scalar(\"f1_score_BlackGrass\", f1_score_BlackGrass[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        \n",
    "        # for multiple GPUs\n",
    "        # optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)\n",
    "        \n",
    "        # for batch normalization, tell tensorflow update batch normalization mean and variance\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='fine_tune'), global_step=tf.train.get_global_step())\n",
    "            \n",
    "        # normal version\n",
    "        #train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "        # monitor trianing information\n",
    "        logging_hook = tf.train.LoggingTensorHook({\"loss\" : loss, \n",
    "                                                   \"accuracy\" : accuracy[1], \n",
    "                                                   \"f1_score_BlackGrass\" : f1_score_BlackGrass[1]}, \n",
    "                                                  every_n_iter=SAVE_SUMMARY_STEPS)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=[logging_hook])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Multiple GPU (Parallel Computing)\n",
    "Testing, not stable version  \n",
    "Evaluation is not yet distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_strategy(num_gpus, all_reduce_alg=None):\n",
    "    \"\"\"Return a DistributionStrategy for running the model.\n",
    "    Args:\n",
    "    num_gpus: Number of GPUs to run this model.\n",
    "    all_reduce_alg: Specify which algorithm to use when performing all-reduce.\n",
    "      See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.\n",
    "      If None, DistributionStrategy will choose based on device topology.\n",
    "    Returns:\n",
    "    tf.contrib.distribute.DistibutionStrategy object.\n",
    "    \"\"\"\n",
    "    if num_gpus == 0:\n",
    "        return tf.contrib.distribute.OneDeviceStrategy(\"device:CPU:0\")\n",
    "    elif num_gpus == 1:\n",
    "        return tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")\n",
    "    else:\n",
    "        if all_reduce_alg:\n",
    "            return tf.contrib.distribute.MirroredStrategy(\n",
    "                num_gpus=num_gpus,\n",
    "                cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\n",
    "                    all_reduce_alg, num_packs=num_gpus))\n",
    "        else:\n",
    "            return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Runing Config\n",
    "Load pretrained weight\n",
    "```python\n",
    "ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=\"pretrained_model\",\n",
    "                                    vars_to_warm_start=\"vgg16.*\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_evaluation_master': '', '_global_id_in_cluster': 0, '_tf_random_seed': 2, '_log_step_count_steps': 100, '_device_fn': None, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_task_id': 0, '_master': '', '_save_checkpoints_secs': None, '_keep_checkpoint_max': 5, '_model_dir': 'model_transfer/', '_task_type': 'worker', '_save_checkpoints_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5c3b979da0>, '_save_summary_steps': 100, '_train_distribute': None, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.8\n",
      "  allow_growth: true\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "session_config = tf.ConfigProto()\n",
    "session_config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session_config.gpu_options.allow_growth = True\n",
    "config = tf.estimator.RunConfig(model_dir=model_dir, \n",
    "                                tf_random_seed=seed, \n",
    "                                save_summary_steps=SAVE_SUMMARY_STEPS, \n",
    "                                save_checkpoints_steps=SAVE_SUMMARY_STEPS, \n",
    "                                session_config=session_config,\n",
    "                                keep_checkpoint_max=5, \n",
    "                                log_step_count_steps=100, )\n",
    "#                                train_distribute=get_distribution_strategy(NUM_GPUS)) #for mutiple GPUs\n",
    "ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=\"pretrained_model\",\n",
    "                                    vars_to_warm_start=\"vgg16.*\")\n",
    "clf = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, config=config, warm_start_from=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "output feature: (?, 128, 128, 3)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='pretrained_model', vars_to_warm_start='vgg16.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('pretrained_model',)\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block5_conv2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block1_conv2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block5_conv2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block4_conv3/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block1_conv1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block4_conv3/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block3_conv2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block1_conv2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block5_conv1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block3_conv1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block3_conv3/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block5_conv1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block3_conv1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block3_conv3/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block4_conv1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block5_conv3/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block2_conv2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block2_conv2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block4_conv2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block1_conv1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block4_conv2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block5_conv3/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block3_conv2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block2_conv1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block2_conv1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: vgg16/block4_conv1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.2870464, step = 0\n",
      "INFO:tensorflow:accuracy = 0.0703125, f1_score_BlackGrass = nan, loss = 3.2870464\n",
      "INFO:tensorflow:Saving checkpoints for 100 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:14:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:14:12\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.275, f1_score_BlackGrass = 0.30769230769230765, global_step = 100, loss = 2.1624618\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: model_transfer/model.ckpt-100\n",
      "INFO:tensorflow:global_step/sec: 0.964479\n",
      "INFO:tensorflow:loss = 2.1260378, step = 100 (103.685 sec)\n",
      "INFO:tensorflow:accuracy = 0.1640625, f1_score_BlackGrass = 0.19512195121951217, loss = 2.1260378 (103.685 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.973708\n",
      "INFO:tensorflow:loss = 1.922525, step = 200 (102.701 sec)\n",
      "INFO:tensorflow:accuracy = 0.22526042, f1_score_BlackGrass = 0.18461538461538463, loss = 1.922525 (102.700 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.980774\n",
      "INFO:tensorflow:loss = 1.8253183, step = 300 (101.960 sec)\n",
      "INFO:tensorflow:accuracy = 0.2626953, f1_score_BlackGrass = 0.14634146341463414, loss = 1.8253183 (101.961 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:19:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:19:19\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.45833334, f1_score_BlackGrass = 0.33333333333333337, global_step = 400, loss = 1.5459541\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: model_transfer/model.ckpt-400\n",
      "INFO:tensorflow:global_step/sec: 0.976796\n",
      "INFO:tensorflow:loss = 1.7142237, step = 400 (102.375 sec)\n",
      "INFO:tensorflow:accuracy = 0.28828126, f1_score_BlackGrass = 0.20168067226890757, loss = 1.7142237 (102.375 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.97339\n",
      "INFO:tensorflow:loss = 1.5698833, step = 500 (102.735 sec)\n",
      "INFO:tensorflow:accuracy = 0.31966147, f1_score_BlackGrass = 0.22695035460992907, loss = 1.5698833 (102.735 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.977943\n",
      "INFO:tensorflow:loss = 1.3725435, step = 600 (102.254 sec)\n",
      "INFO:tensorflow:accuracy = 0.34486607, f1_score_BlackGrass = 0.2345679012345679, loss = 1.3725435 (102.254 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 700 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:24:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:24:27\n",
      "INFO:tensorflow:Saving dict for global step 700: accuracy = 0.525, f1_score_BlackGrass = 0.30769230769230765, global_step = 700, loss = 1.2686613\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 700: model_transfer/model.ckpt-700\n",
      "INFO:tensorflow:global_step/sec: 0.968481\n",
      "INFO:tensorflow:loss = 1.4163312, step = 700 (103.255 sec)\n",
      "INFO:tensorflow:accuracy = 0.36767578, f1_score_BlackGrass = 0.24468085106382978, loss = 1.4163312 (103.256 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.961482\n",
      "INFO:tensorflow:loss = 1.2337838, step = 800 (104.005 sec)\n",
      "INFO:tensorflow:accuracy = 0.39019096, f1_score_BlackGrass = 0.26415094339622636, loss = 1.2337838 (104.005 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 900 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.957874\n",
      "INFO:tensorflow:loss = 1.3123281, step = 900 (104.399 sec)\n",
      "INFO:tensorflow:accuracy = 0.40390626, f1_score_BlackGrass = 0.25531914893617025, loss = 1.3123281 (104.399 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:29:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:29:40\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.6, f1_score_BlackGrass = 0.4615384615384615, global_step = 1000, loss = 1.100632\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: model_transfer/model.ckpt-1000\n",
      "INFO:tensorflow:global_step/sec: 0.961327\n",
      "INFO:tensorflow:loss = 1.1646609, step = 1000 (104.022 sec)\n",
      "INFO:tensorflow:accuracy = 0.421875, f1_score_BlackGrass = 0.2629482071713147, loss = 1.1646609 (104.022 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.978131\n",
      "INFO:tensorflow:loss = 1.2187215, step = 1100 (102.236 sec)\n",
      "INFO:tensorflow:accuracy = 0.43424478, f1_score_BlackGrass = 0.25925925925925924, loss = 1.2187215 (102.236 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.955074\n",
      "INFO:tensorflow:loss = 1.0924587, step = 1200 (104.704 sec)\n",
      "INFO:tensorflow:accuracy = 0.45072114, f1_score_BlackGrass = 0.2653061224489796, loss = 1.0924587 (104.702 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:34:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-1300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:34:49\n",
      "INFO:tensorflow:Saving dict for global step 1300: accuracy = 0.64166665, f1_score_BlackGrass = 0.4615384615384615, global_step = 1300, loss = 0.97875196\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1300: model_transfer/model.ckpt-1300\n",
      "INFO:tensorflow:global_step/sec: 0.972838\n",
      "INFO:tensorflow:loss = 1.1082114, step = 1300 (102.792 sec)\n",
      "INFO:tensorflow:accuracy = 0.46177456, f1_score_BlackGrass = 0.2803738317757009, loss = 1.1082114 (102.792 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.955717\n",
      "INFO:tensorflow:loss = 1.0363725, step = 1400 (104.634 sec)\n",
      "INFO:tensorflow:accuracy = 0.475, f1_score_BlackGrass = 0.28152492668621704, loss = 1.0363725 (104.634 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.989055\n",
      "INFO:tensorflow:loss = 1.0449566, step = 1500 (101.106 sec)\n",
      "INFO:tensorflow:accuracy = 0.48486328, f1_score_BlackGrass = 0.2928176795580111, loss = 1.0449566 (101.106 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1600 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:39:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-1600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:40:00\n",
      "INFO:tensorflow:Saving dict for global step 1600: accuracy = 0.65833336, f1_score_BlackGrass = 0.4615384615384615, global_step = 1600, loss = 0.9233831\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1600: model_transfer/model.ckpt-1600\n",
      "INFO:tensorflow:global_step/sec: 0.955859\n",
      "INFO:tensorflow:loss = 0.9509857, step = 1600 (104.619 sec)\n",
      "INFO:tensorflow:accuracy = 0.49586397, f1_score_BlackGrass = 0.2989690721649485, loss = 0.9509857 (104.620 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1700 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:global_step/sec: 0.981683\n",
      "INFO:tensorflow:loss = 0.95755494, step = 1700 (101.864 sec)\n",
      "INFO:tensorflow:accuracy = 0.50716144, f1_score_BlackGrass = 0.3106796116504854, loss = 0.95755494 (101.864 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into model_transfer/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (300 secs).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-06:43:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-1800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-06:43:26\n",
      "INFO:tensorflow:Saving dict for global step 1800: accuracy = 0.675, f1_score_BlackGrass = 0.33333333333333337, global_step = 1800, loss = 0.90106404\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1800: model_transfer/model.ckpt-1800\n",
      "INFO:tensorflow:Loss for final step: 0.87980604.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.675,\n",
       "  'f1_score_BlackGrass': 0.33333333333333337,\n",
       "  'global_step': 1800,\n",
       "  'loss': 0.90106404},\n",
       " [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spec = tf.estimator.TrainSpec(lambda:train_input_fn_keras(generator), max_steps=1800)\n",
    "eval_spec = tf.estimator.EvalSpec(lambda:eval_input_fn(valid_dataset, valid_labels), throttle_secs=300)\n",
    "tf.estimator.train_and_evaluate(clf, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the pretrained weights are freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHJZJREFUeJzt3XucXGWd5/HPl4SbcgmXNoQk0ChhEHAN2oO4jityUS6OwR1FWJSI0TAKOzrqjkHdFV2ZhVmV0ZcuGoUheCNRuUQug4BcvAVoNCIEhQDBdAxJA7kQEdbgb/54njaHprrrVN8qefJ9v1716nN5zjm/U1X9rVNPnTqliMDMzMq1TbsLMDOz0eWgNzMrnIPezKxwDnozs8I56M3MCuegNzMrnIO+DSSdI+mb7a6jStJnJD0m6dEx2t69ko4Yi22VStI+kjZIGlejbaekkDR+LGobSZJukfSeUVjvuyT9ZKyXbQcHvSFpH+DDwEERsddohEJe57K+8Yg4OCJuGan1t5ukIyT19Jt2jqRzRmubEfG7iNgpIp4d7ro2x4MPGzkOegPYB3g8IlaPxMq2xKNGGxt+brTHVh/0kk6X9IPK+AOSvlsZXy5peoPlrpN0Vr9pv5L0X/PwF/Ky6yXdJem1A2y/0ZHgMklH5+FtJM2R9KCkxyUtkLR7nreDpG/m6Wsl3Slp4gDb6VvHk5KWSHpLnn40cAOwd+4GuAS4LS+2Nk97dW77bkn3SVoj6XpJ+1bWH5LOlPQA8EDje3vAfTxH0nfzvjwp6deSDpB0tqTV+X58Q2XZ03MdT0p6SNIZ/db9T5JWSvq9pPfk2vbP87aX9FlJv5O0StJXJO1YfSwkfThvd6Wk0yvrbbispBcC11Xuww2S9m6y/49IemUePjXXeHAenyXpyjw82OP/nHdekvaTdFu+X26U9OUGR+mn5vofk/TxvNyxwMeAt+faf5Wnvyvfv09KeljSqQPsy2GSfp6fgyslfUnSdpX5z3tuSDpQ0g2SnpD0W0knDXZ/Vda1jaRP5PtvtaRLJe1amf9dSY9KWpfvi4Mr8/aQtFDpf/IO4CX91j1gTc2W3exFxFZ9A14MrCW96O0NPAL0VOatAbZpsNxpwE8r4wfl9Wyfx98B7AGMJ3WLPArskOedA3wzDx/Rt73KupYBR+fhDwCLgCnA9sBXge/keWcAPwBeAIwDXgnsMsB+vi3v3zbA24E/AJMa1QB0AgGMr0ybASwFXpr36RPAzyrzg/SCsTuwY437vbqP5wBPA2/M674UeBj4OLAt8F7g4cqyJ5D+0QS8DngKeEWed2y+rw/O98s3c2375/kXAAtznTvn++//VO6HjcCn83aPz+vereayPc32u7IPlwIfzsNzgQeB91Xm/WONx/85jxPwc+CzwHbA3wDr2fQ862v7NWBH4OXAM8BL+z8n8/gL8/J/lccnAQcPsC+vBA7Pj10ncB/wwYGeG3ndy4HT8zKHAo+Rug4brf8W4D15+N2k5+GLgZ2Ay4FvVNq+Oz822wP/CiyuzLsMWJC3fwiwAvhJZX8HrGmwZbeEW9sL2Bxu+QF+BXBy/qe7AzgwP+gLB1hmZ1JY7pvHzwUuHmQba4CX5+G//FPRPOjvA46qzJsE/Ck/Gd8N/Az4T0PY58XAjEY10DjorwNmVca3IYVg3/4HcGQL26/u4znADZV5fwtsAMZV7usAJgywriuBD+Thi8nhm8f3z8vuT3ph+APwksr8V5NfRPL98Md++72aFGJ1lm0l6Gf1PbfyY/we4LI8/gibXrgGe/z/8jiRut82Ai+otP0mzw/6KZX5dwAn939O5vEXkg5c/o4aL9z99u2DwBWV8ec8N0gHGj/ut8xXgU8OsL5b2BT0NwHvr8z7q777o8FyE/K2dyUdCP0JOLAy/5/ZFPQD1tRs2S3httV33WS3kv5R/0sevoV0pPi6PP48EfEkcA3pxQHgFOBbffMlfSR3L6yTtJb0ZNtzCLXtC1yR3xavJf3jPwtMBL4BXA9clrsp/kXSto1WIuk0SYsr6zmkxXr2Bb5QWf4JUvhNrrRZ3vLebbKqMvxH4LHY9CHjH/PfnQAkHSdpUX6LvZZ05N23L3v3q6M63EE6yr+rsh//nqf3eTwiNlbGn8rbrbNsK24FXitpEilIFgCvkdRJeq4szu0Ge/yr9gaeiIinKtMaPR7Vs6r69u15IuIPpPD7e2ClpGskHdiorVI329W5y2Q9KQT7P7eqtewLvKpvn/J+nQrs1Wj9/fS96+7zCOmFbqKkcZLOy91c60kHE+RaOnK75f2WrVNTs2U3ew76pC/oX5uHb6VJ0GffAU5R6sPeAbgZQKk//p+Ak0hv+ycA60jB2N8fSAFCXnYczw2P5cBxETGhctshIlZExJ8i4lMRcRDwn4E3kbqUnkOpL/1rwFnAHrmeewaoB9JRUH/LgTP61bFjRPysyXIjStL2wPdJXRQT875cy6Z9WUnq5ugztTL8GOlF4+DKPuwaEQ3Drp9my7a07xGxlBS0/x24LSLWk0J4NulI8c+56YCPf79VrgR2l/SCyrSp1Pe8+iPi+og4hvQu4jek51AjF+b50yJiF1J/f//nVnX9y4Fb++3TThHxvhp1/p4Uyn363smsAv4bqYvxaNKLZWduI6A3t5vab9k6NTVbdrPnoE9uBV5PeovaA/yY1Ne7B/DLQZa7lvSk+zQwv/LPuTPpidELjJf0v4BdBljH/cAOkk7IR+OfIPUv9vkKcG4OayR1SJqRh18v6WX5xWE96e3ln3m+F5L+0XrzcqeTjugH0pvX8+J+dZytTR8Y7irpbYOsY7RsR7p/eoGNko4D3lCZvwA4XdJLc+j9z74Z+fH5GnCBpBcBSJos6Y3NNlpj2VXAHtUPBmu4lfTi23cwcUu/cRjk8e9X3yNAN3COpO3ywcfftlDLKqBT0jZ5OxMlzVD6oPkZUldao+cWpOf7emBDPupvFthXAwdIeqekbfPtryW9tEad3wH+UemD551I7x7m53dhO+daHycdPP1z30L53eHlpPvnBZIOAmbWqanGsps9Bz0QEfeTnsg/zuPrgYdIH7YOeI5yRDxDegIcDXy7Mut60tv6+0lv8Z5mgG6NiFgHvB/4OukDnj8A1bNwvkD6APCHkp4kfTD3qjxvL+B7pH+y+0gB8Y0G21gCfI70Yd0q4GXATwfZr6dInzn8NL+NPTwirgDOJ3UTrSe9IzhuoHWMltxl9g+kQF9DOopbWJl/HfBF0rurpaT7C1IAAHy0b3rejxtJ/bx1DLhsRPyGFEIP5fts0LNusltJ4XTbAOMw+OPf36mkzw0eBz4DzGfTfjfTd6bZ45J+QcqGD5GOoJ8gvbsdKMA/QnocniS9GM4fbEP5MXwDqdvz96R3Mufz3AOcgVxMeo7fRvrA/mnSuyJIH2I/Qvo/WsKmx77PWaSuqkeBS4B/a6GmAZfdEih/sGBWpHyUeA/pbKiNzdqXRNJ84DcR8cl212Lt5SN6K46ktyid874b6ajsB1tDyOeuhpconWt+LKm/+sp212Xt56C3Ep1BOi3yQdIZKnU+5CvBXqR+/g2k7qv3RcRgnzHZVsJdN2ZmhfMRvZlZ4TaLCwztueee0dnZ2e4yzMy2KHfddddjEdH0S3ubRdB3dnbS3d3d7jLMzLYokmp9Q9ddN2Zmhasd9Pk6Er+UdHUe30/S7ZKWSpqvfFnSfFrb/Dz99nztDjMza5NWjug/QPr2ZZ/zgQsiYn/SNxRn5emzgDV5+gW5nZmZtUmtoJc0hXQN8K/ncQFHkr5+DzAPODEPz8jj5PlH5fZmZtYGdY/o/5V0Nca+ixrtAaytfNuwh02Xq51Mvq5Lnr8ut38OSbMldUvq7u3tHWL5ZmbWTNOgl/QmYHVE3DWSG46IuRHRFRFdHR1DvaS3mZk1U+f0ytcAb5Z0POma67uQrqg3QdL4fNQ+hXTFOPLfqUCP0m9Z7kq6mp6ZmbVB0yP6iDg7IqZERCfpEp4/iohTSZeBfWtuNhO4Kg8vZNO1mt+a2/s6C2ZmbTKc8+g/CnxI0lJSH/xFefpFpB9gWEq6nvWc4ZVoZmbD0dI3YyPiFtLV8YiIh4DDGrR5GmjHLw9ZwTrnXNO2bS8774S2bdtsJPibsWZmhXPQm5kVzkFvZlY4B72ZWeEc9GZmhXPQm5kVzkFvZlY4B72ZWeEc9GZmhXPQm5kVzkFvZlY4B72ZWeEc9GZmhXPQm5kVzkFvZla4lq5Hb2Zjp13X4Pf198vjI3ozs8I1DXpJO0i6Q9KvJN0r6VN5+iWSHpa0ON+m5+mS9EVJSyXdLekVo70TZmY2sDpdN88AR0bEBknbAj+RdF2e9z8i4nv92h8HTMu3VwEX5r9mZtYGTY/oI9mQR7fNtxhkkRnApXm5RcAESZOGX6qZmQ1FrT56SeMkLQZWAzdExO151rm5e+YCSdvnaZOB5ZXFe/K0/uucLalbUndvb+8wdsHMzAZTK+gj4tmImA5MAQ6TdAhwNnAg8NfA7sBHW9lwRMyNiK6I6Oro6GixbDMzq6uls24iYi1wM3BsRKzM3TPPAP8GHJabrQCmVhabkqeZmVkb1DnrpkPShDy8I3AM8Ju+fndJAk4E7smLLAROy2ffHA6si4iVo1K9mZk1Veesm0nAPEnjSC8MCyLiakk/ktQBCFgM/H1ufy1wPLAUeAo4feTLNhs77friktlIaRr0EXE3cGiD6UcO0D6AM4dfmpmZjQR/M9bMrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwtX5cfAdJN0h6VeS7pX0qTx9P0m3S1oqab6k7fL07fP40jy/c3R3wczMBlPniP4Z4MiIeDkwHThW0uHA+cAFEbE/sAaYldvPAtbk6RfkdmZm1iZNgz6SDXl023wL4Ejge3n6PODEPDwjj5PnHyVJI1axmZm1pFYfvaRxkhYDq4EbgAeBtRGxMTfpASbn4cnAcoA8fx2wR4N1zpbULam7t7d3eHthZmYDqhX0EfFsREwHpgCHAQcOd8MRMTciuiKiq6OjY7irMzOzAbR01k1ErAVuBl4NTJA0Ps+aAqzIwyuAqQB5/q7A4yNSrZmZtazOWTcdkibk4R2BY4D7SIH/1txsJnBVHl6Yx8nzfxQRMZJFm5lZfeObN2ESME/SONILw4KIuFrSEuAySZ8BfglclNtfBHxD0lLgCeDkUajbzMxqahr0EXE3cGiD6Q+R+uv7T38aeNuIVGdmZsPmb8aamRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRWuzm/GTpV0s6Qlku6V9IE8/RxJKyQtzrfjK8ucLWmppN9KeuNo7oCZmQ2uzm/GbgQ+HBG/kLQzcJekG/K8CyLis9XGkg4i/U7swcDewI2SDoiIZ0eycDMzq6fpEX1ErIyIX+ThJ4H7gMmDLDIDuCwinomIh4GlNPhtWTMzGxt1juj/QlIn6YfCbwdeA5wl6TSgm3TUv4b0IrCoslgPDV4YJM0GZgPss88+Qyjd2qFzzjXtLsHMWlT7w1hJOwHfBz4YEeuBC4GXANOBlcDnWtlwRMyNiK6I6Oro6GhlUTMza0GtoJe0LSnkvxURlwNExKqIeDYi/gx8jU3dMyuAqZXFp+RpZmbWBnXOuhFwEXBfRHy+Mn1SpdlbgHvy8ELgZEnbS9oPmAbcMXIlm5lZK+r00b8GeCfwa0mL87SPAadImg4EsAw4AyAi7pW0AFhCOmPnTJ9xY2bWPk2DPiJ+AqjBrGsHWeZc4Nxh1GVmZiPE34w1Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHB1fhx8qqSbJS2RdK+kD+Tpu0u6QdID+e9uebokfVHSUkl3S3rFaO+EmZkNrM4R/UbgwxFxEHA4cKakg4A5wE0RMQ24KY8DHAdMy7fZwIUjXrWZmdXWNOgjYmVE/CIPPwncB0wGZgDzcrN5wIl5eAZwaSSLgAmSJo145WZmVktLffSSOoFDgduBiRGxMs96FJiYhycDyyuL9eRp/dc1W1K3pO7e3t4WyzYzs7pqB72knYDvAx+MiPXVeRERQLSy4YiYGxFdEdHV0dHRyqJmZtaCWkEvaVtSyH8rIi7Pk1f1dcnkv6vz9BXA1MriU/I0MzNrgzpn3Qi4CLgvIj5fmbUQmJmHZwJXVaafls++ORxYV+niMTOzMTa+RpvXAO8Efi1pcZ72MeA8YIGkWcAjwEl53rXA8cBS4Cng9BGt2MzMWtI06CPiJ4AGmH1Ug/YBnDnMuszMbIT4m7FmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVr+gtTki4G3gSsjohD8rRzgPcCvbnZxyLi2jzvbGAW8CzwDxFx/SjUbWajpHPONW3b9rLzTmjbtktW54j+EuDYBtMviIjp+dYX8gcBJwMH52X+n6RxI1WsmZm1rmnQR8RtwBM11zcDuCwinomIh0k/EH7YMOozM7NhGk4f/VmS7pZ0saTd8rTJwPJKm5487XkkzZbULam7t7e3URMzMxsBQw36C4GXANOBlcDnWl1BRMyNiK6I6Oro6BhiGWZm1syQgj4iVkXEsxHxZ+BrbOqeWQFMrTSdkqeZmVmbDCnoJU2qjL4FuCcPLwROlrS9pP2AacAdwyvRzMyGo87pld8BjgD2lNQDfBI4QtJ0IIBlwBkAEXGvpAXAEmAjcGZEPDs6pZuZWR1Ngz4iTmkw+aJB2p8LnDucoszMbOT4m7FmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFa7Ob8ZeDLwJWB0Rh+RpuwPzgU7Sb8aeFBFrJAn4AnA88BTwroj4xeiUvvXqnHNNu0swsy1InSP6S4Bj+02bA9wUEdOAm/I4wHHAtHybDVw4MmWamdlQNQ36iLgNeKLf5BnAvDw8DzixMv3SSBYBEyRNGqlizcysdUPto58YESvz8KPAxDw8GVheadeTpz2PpNmSuiV19/b2DrEMMzNrZtgfxkZEADGE5eZGRFdEdHV0dAy3DDMzG8BQg35VX5dM/rs6T18BTK20m5KnmZlZmww16BcCM/PwTOCqyvTTlBwOrKt08ZiZWRvUOb3yO8ARwJ6SeoBPAucBCyTNAh4BTsrNryWdWrmUdHrl6aNQs5mZtaBp0EfEKQPMOqpB2wDOHG5RZmY2cvzNWDOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHBNL2pmZjZW2vXD98vOO6Et2x0rPqI3Myucg97MrHAOejOzwjnozcwKN6wPYyUtA54EngU2RkSXpN2B+UAnsAw4KSLWDK9MMzMbqpE4on99REyPiK48Pge4KSKmATflcTMza5PR6LqZAczLw/OAE0dhG2ZmVtNwgz6AH0q6S9LsPG1iRKzMw48CExstKGm2pG5J3b29vcMsw8zMBjLcL0z9TUSskPQi4AZJv6nOjIiQFI0WjIi5wFyArq6uhm3MzGz4hnVEHxEr8t/VwBXAYcAqSZMA8t/Vwy3SzMyGbshBL+mFknbuGwbeANwDLARm5mYzgauGW6SZmQ3dcLpuJgJXSOpbz7cj4t8l3QkskDQLeAQ4afhlmpnZUA056CPiIeDlDaY/Dhw1nKLMzGzk+JuxZmaF82WKh6Fdl1Q1M2uFj+jNzArnoDczK5yD3syscA56M7PCOejNzArnoDczK5yD3syscA56M7PCOejNzArnoDczK5yD3syscL7WjZlt9dp53apl550w6tvwEb2ZWeEc9GZmhXPQm5kVzkFvZla4UQt6ScdK+q2kpZLmjNZ2zMxscKNy1o2kccCXgWOAHuBOSQsjYslIb8u/8mRmNrjROqI/DFgaEQ9FxP8HLgNmjNK2zMxsEKN1Hv1kYHllvAd4VbWBpNnA7Dy6QdJvR6mWOvYEHmvj9ofL9bfPllw7bNn1b8m1Q65f5w9rHfvWadS2L0xFxFxgbru2XyWpOyK62l3HULn+9tmSa4ctu/4tuXYY2/pHq+tmBTC1Mj4lTzMzszE2WkF/JzBN0n6StgNOBhaO0rbMzGwQo9J1ExEbJZ0FXA+MAy6OiHtHY1sjZLPoQhoG198+W3LtsGXXvyXXDmNYvyJirLZlZmZt4G/GmpkVzkFvZla4rTLoJe0u6QZJD+S/uw3Qbh9JP5R0n6QlkjrHttLG6taf2+4iqUfSl8ayxsHUqV/SdEk/l3SvpLslvb0dtVbqGfSSHpK2lzQ/z799c3muQK3aP5Sf33dLuklSrXOzx0rdy6lI+jtJIWmzOeWyTu2STsr3/72Svj0qhUTEVncD/gWYk4fnAOcP0O4W4Jg8vBPwgnbX3kr9ef4XgG8DX2p33a3UDxwATMvDewMrgQltqncc8CDwYmA74FfAQf3avB/4Sh4+GZjf7vu5hdpf3/fcBt63udRet/7cbmfgNmAR0NXuulu476cBvwR2y+MvGo1atsojetLlGObl4XnAif0bSDoIGB8RNwBExIaIeGrsShxU0/oBJL0SmAj8cIzqqqtp/RFxf0Q8kId/D6wGOsaswueqc0mP6j59DzhKksawxoE0rT0ibq48txeRvveyuah7OZX/DZwPPD2WxTVRp/b3Al+OiDUAEbF6NArZWoN+YkSszMOPksKwvwOAtZIul/RLSf83X6xtc9C0fknbAJ8DPjKWhdVU5/7/C0mHkY6IHhztwgbQ6JIekwdqExEbgXXAHmNS3eDq1F41C7huVCtqTdP6Jb0CmBoRm9sVDuvc9wcAB0j6qaRFko4djUKK/c1YSTcCezWY9fHqSESEpEbnmI4HXgscCvwOmA+8C7hoZCttbATqfz9wbUT0tOPAcgTq71vPJOAbwMyI+PPIVmlVkt4BdAGva3ctdeUDms+T/je3RONJ3TdHkN5J3SbpZRGxdqQ3UqSIOHqgeZJWSZoUEStzkDR6u9QDLI6Ih/IyVwKHM0ZBPwL1vxp4raT3kz5f2E7ShogYk98GGIH6kbQLcA3w8YhYNEql1lHnkh59bXokjQd2BR4fm/IGVetyJJKOJr0Ivy4inhmj2upoVv/OwCHALfmAZi9goaQ3R0T3mFXZWJ37vge4PSL+BDws6X5S8N85koVsrV03C4GZeXgmcFWDNncCEyT19QsfCYz49fSHqGn9EXFqROwTEZ2k7ptLxyrka2haf750xhWkur83hrU1UueSHtV9eivwo8ifrrVZ09olHQp8FXjzaPURD8Og9UfEuojYMyI683N9EWk/2h3yUO95cyXpaB5Je5K6ch4a8Ura/cl0O26kvtObgAeAG4Hd8/Qu4OuVdscAdwO/Bi4Btmt37a3UX2n/Ljavs26a1g+8A/gTsLhym97Gmo8H7id9TvDxPO3TpFAB2AH4LrAUuAN4cbvv5xZqvxFYVbmfF7a75lbq79f2FjaTs25q3vcidT0tyTlz8mjU4UsgmJkVbmvtujEz22o46M3MCuegNzMrnIPezKxwDnozs8I56M3MCuegNzMr3H8AlMgQvE7QF9wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "with tf.Session() as sess:\n",
    "    vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "    w = vgg16.get_layer('block1_conv1').get_weights()[0]\n",
    "    plt.hist(w.flatten())\n",
    "    plt.title(\"w values after 'imagenet' weights are loaded\")\n",
    "    plt.show()\n",
    "\n",
    "w = clf.get_variable_value(\"vgg16/block1_conv1/kernel\")\n",
    "print(w.shape)\n",
    "plt.hist(w.flatten())\n",
    "plt.title(\"w values after 'imagenet' weights are loaded\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model for Tensorflow Serving\n",
    "Can not save model after **predict**, because `Graph` is finalized and cannot be modified  \n",
    "You can assign which model to be saved by `checkpoint_path` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['classify', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-1800\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: saved_model/temp-b'1536907624'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'saved_model/1536907624'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input\n",
    "image = tf.placeholder(tf.float32, shape=[None, 128, 128, 3], name='image')\n",
    "# input receiver\n",
    "input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
    "    'image': image,\n",
    "})\n",
    "\n",
    "clf.export_savedmodel(\"saved_model/\", input_fn, checkpoint_path=\"model_transfer/model.ckpt-1800\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and predict\n",
    "Estimator predict method return **generator** type, so if you want to get all predictions please use for loop  \n",
    "```python\n",
    "for result in results:\n",
    "    print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_transfer/model.ckpt-1800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_ids': array([8]),\n",
       " 'logits': array([-2.2231734 , -0.7330439 ,  0.63402456, -0.8422852 , -1.5529217 ,\n",
       "        -0.67170465, -1.6516896 , -0.53993213,  2.340041  ,  1.1269958 ,\n",
       "         0.6424718 , -1.3946064 ], dtype=float32),\n",
       " 'probability': array([0.00540818, 0.02399974, 0.09417091, 0.0215161 , 0.01057154,\n",
       "        0.02551795, 0.00957731, 0.02911212, 0.5185973 , 0.15417412,\n",
       "        0.09496976, 0.01238493], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = clf.predict(lambda: eval_input_fn(valid_dataset, valid_labels), checkpoint_path=\"model_transfer/model.ckpt-1800\")\n",
    "next(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from `Estimator.export_savedmodel`\n",
    "Reference: https://qiita.com/parkkiung123/items/13adb482860f356f97f3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model/1536907624/variables/variables\n",
      "[ 8  8  8  8  8  6  8  6  8  6  4 11  7 11 11 11  1  7  5 11  4  6  6  6\n",
      "  4  4  4  4  6  6  1  1  1  1  1  1  1  1  1  1 10 10 10 10 10 10 10 10\n",
      " 10 10  6  6  6  6  6  6  6  6  6  6  7  3  7  7  7  7  7  7  7  7  9  8\n",
      "  3 10  9  9  8  9  8 10  3  3  3  3  3  3  3  8  3  3  6  6  5  1  5  5\n",
      "  1  5  1  1  2  3  2  3  2  5  2  2  2  7  6  6  6  6  4  0  0  6  6  6]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "export_dir = 'saved_model/1536907624'\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # saved_model load\n",
    "    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n",
    "    # input\n",
    "    i = sess.graph.get_tensor_by_name(\"image:0\")\n",
    "    # output\n",
    "    r = sess.graph.get_tensor_by_name(\"ArgMax:0\")\n",
    "    print(sess.run(r, feed_dict={i:valid_dataset}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Visualization\n",
    "Reference: https://github.com/InFoCusp/tf_cnnvis  \n",
    "Although you put multiple images, it only draws one image for visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model/1536907624/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from model/tmp-model\n",
      "Reconstruction Completed for vgg16/block1_conv1/Relu layer. Time taken = 0.291235 s\n",
      "Reconstruction Completed for vgg16/block1_conv2/Relu layer. Time taken = 0.253113 s\n",
      "Reconstruction Completed for vgg16/block2_conv1/Relu layer. Time taken = 0.122363 s\n",
      "Reconstruction Completed for vgg16/block2_conv2/Relu layer. Time taken = 0.142927 s\n",
      "Reconstruction Completed for vgg16/block3_conv1/Relu layer. Time taken = 0.078881 s\n",
      "Reconstruction Completed for vgg16/block3_conv2/Relu layer. Time taken = 0.095986 s\n",
      "Reconstruction Completed for vgg16/block3_conv3/Relu layer. Time taken = 0.072141 s\n",
      "Reconstruction Completed for vgg16/block4_conv1/Relu layer. Time taken = 0.074723 s\n",
      "Reconstruction Completed for vgg16/block4_conv2/Relu layer. Time taken = 0.086658 s\n",
      "Reconstruction Completed for vgg16/block4_conv3/Relu layer. Time taken = 0.056931 s\n",
      "Reconstruction Completed for vgg16/block5_conv1/Relu layer. Time taken = 0.068906 s\n",
      "Reconstruction Completed for vgg16/block5_conv2/Relu layer. Time taken = 0.056887 s\n",
      "Reconstruction Completed for vgg16/block5_conv3/Relu layer. Time taken = 0.054189 s\n",
      "Reconstruction Completed for fine_tune/dense1/Relu layer. Time taken = 0.052124 s\n",
      "Reconstruction Completed for fine_tune/dense2/Relu layer. Time taken = 0.039585 s\n",
      "Reconstruction Completed for vgg16/block1_pool/MaxPool layer. Time taken = 0.056941 s\n",
      "Reconstruction Completed for vgg16/block2_pool/MaxPool layer. Time taken = 0.054470 s\n",
      "Reconstruction Completed for vgg16/block3_pool/MaxPool layer. Time taken = 0.056667 s\n",
      "Reconstruction Completed for vgg16/block4_pool/MaxPool layer. Time taken = 0.051883 s\n",
      "Reconstruction Completed for vgg16/block5_pool/MaxPool layer. Time taken = 0.046494 s\n",
      "Reconstruction Completed for vgg16/block1_conv1/Conv2D layer. Time taken = 0.302271 s\n",
      "Reconstruction Completed for vgg16/block1_conv2/Conv2D layer. Time taken = 0.248561 s\n",
      "Reconstruction Completed for vgg16/block2_conv1/Conv2D layer. Time taken = 0.103122 s\n",
      "Reconstruction Completed for vgg16/block2_conv2/Conv2D layer. Time taken = 0.099080 s\n",
      "Reconstruction Completed for vgg16/block3_conv1/Conv2D layer. Time taken = 0.064700 s\n",
      "Reconstruction Completed for vgg16/block3_conv2/Conv2D layer. Time taken = 0.060625 s\n",
      "Reconstruction Completed for vgg16/block3_conv3/Conv2D layer. Time taken = 0.062207 s\n",
      "Reconstruction Completed for vgg16/block4_conv1/Conv2D layer. Time taken = 0.063865 s\n",
      "Reconstruction Completed for vgg16/block4_conv2/Conv2D layer. Time taken = 0.058478 s\n",
      "Reconstruction Completed for vgg16/block4_conv3/Conv2D layer. Time taken = 0.057464 s\n",
      "Reconstruction Completed for vgg16/block5_conv1/Conv2D layer. Time taken = 0.049684 s\n",
      "Reconstruction Completed for vgg16/block5_conv2/Conv2D layer. Time taken = 0.054026 s\n",
      "Reconstruction Completed for vgg16/block5_conv3/Conv2D layer. Time taken = 0.048042 s\n",
      "Total Time = 3.488610\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tf_cnnvis import tf_cnnvis\n",
    "\n",
    "# activation visualization\n",
    "layers = ['r', 'p', 'c']\n",
    "\n",
    "start = time.time()\n",
    "image = sess.graph.get_tensor_by_name(\"image:0\")\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n",
    "    # with sess_graph_path = None, the default Session will be used for visualization.\n",
    "    is_success = tf_cnnvis.activation_visualization(sess_graph_path = None, value_feed_dict = {image : valid_dataset[10:11]}, \n",
    "                                                    layers=layers, path_logdir=os.path.join(\"Log\",\"VGGNet\"), \n",
    "                                                    path_outdir=os.path.join(\"Output\",\"VGGNet\"))\n",
    "start = time.time() - start\n",
    "print(\"Total Time = %f\" % (start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model/1536907624/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from model/tmp-model\n",
      "Reconstruction Completed for vgg16/block1_conv1/Relu layer. Time taken = 0.548123 s\n",
      "Reconstruction Completed for vgg16/block1_conv2/Relu layer. Time taken = 0.899879 s\n",
      "Reconstruction Completed for vgg16/block2_conv1/Relu layer. Time taken = 1.293494 s\n",
      "Reconstruction Completed for vgg16/block2_conv2/Relu layer. Time taken = 1.719456 s\n",
      "Reconstruction Completed for vgg16/block3_conv1/Relu layer. Time taken = 2.905643 s\n",
      "Reconstruction Completed for vgg16/block3_conv2/Relu layer. Time taken = 3.238812 s\n",
      "Reconstruction Completed for vgg16/block3_conv3/Relu layer. Time taken = 3.525879 s\n",
      "Reconstruction Completed for vgg16/block4_conv1/Relu layer. Time taken = 6.416328 s\n",
      "Reconstruction Completed for vgg16/block4_conv2/Relu layer. Time taken = 6.691745 s\n",
      "Reconstruction Completed for vgg16/block4_conv3/Relu layer. Time taken = 6.492911 s\n",
      "Reconstruction Completed for vgg16/block5_conv1/Relu layer. Time taken = 7.238698 s\n",
      "Reconstruction Completed for vgg16/block5_conv2/Relu layer. Time taken = 6.859071 s\n",
      "Reconstruction Completed for vgg16/block5_conv3/Relu layer. Time taken = 5.958188 s\n",
      "Reconstruction Completed for fine_tune/dense1/Relu layer. Time taken = 3.502145 s\n",
      "Reconstruction Completed for fine_tune/dense2/Relu layer. Time taken = 2.969978 s\n",
      "Reconstruction Completed for vgg16/block1_pool/MaxPool layer. Time taken = 1.672902 s\n",
      "Reconstruction Completed for vgg16/block2_pool/MaxPool layer. Time taken = 2.645831 s\n",
      "Reconstruction Completed for vgg16/block3_pool/MaxPool layer. Time taken = 4.400438 s\n",
      "Reconstruction Completed for vgg16/block4_pool/MaxPool layer. Time taken = 6.997916 s\n",
      "Reconstruction Completed for vgg16/block5_pool/MaxPool layer. Time taken = 6.345478 s\n",
      "Reconstruction Completed for vgg16/block1_conv1/Conv2D layer. Time taken = 1.536746 s\n",
      "Reconstruction Completed for vgg16/block1_conv2/Conv2D layer. Time taken = 2.090142 s\n",
      "Reconstruction Completed for vgg16/block2_conv1/Conv2D layer. Time taken = 2.468435 s\n",
      "Reconstruction Completed for vgg16/block2_conv2/Conv2D layer. Time taken = 2.545311 s\n",
      "Reconstruction Completed for vgg16/block3_conv1/Conv2D layer. Time taken = 3.953956 s\n",
      "Reconstruction Completed for vgg16/block3_conv2/Conv2D layer. Time taken = 4.066882 s\n",
      "Reconstruction Completed for vgg16/block3_conv3/Conv2D layer. Time taken = 4.235821 s\n",
      "Reconstruction Completed for vgg16/block4_conv1/Conv2D layer. Time taken = 7.052501 s\n",
      "Reconstruction Completed for vgg16/block4_conv2/Conv2D layer. Time taken = 7.531119 s\n",
      "Reconstruction Completed for vgg16/block4_conv3/Conv2D layer. Time taken = 7.527126 s\n",
      "Reconstruction Completed for vgg16/block5_conv1/Conv2D layer. Time taken = 8.046511 s\n",
      "Reconstruction Completed for vgg16/block5_conv2/Conv2D layer. Time taken = 8.076717 s\n",
      "Reconstruction Completed for vgg16/block5_conv3/Conv2D layer. Time taken = 8.164468 s\n",
      "Total Time = 150.217149\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tf_cnnvis import tf_cnnvis\n",
    "\n",
    "# deconv visualization\n",
    "layers = ['r', 'p', 'c']\n",
    "\n",
    "start = time.time()\n",
    "image = sess.graph.get_tensor_by_name(\"image:0\")\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n",
    "    is_success = tf_cnnvis.deconv_visualization(sess_graph_path = None, value_feed_dict = {image : valid_dataset[10:11]}, \n",
    "                                                layers=layers, path_logdir=os.path.join(\"Log\",\"VGGNet\"), \n",
    "                                                path_outdir=os.path.join(\"Output\",\"VGGNet\"))\n",
    "start = time.time() - start\n",
    "print(\"Total Time = %f\" % (start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
